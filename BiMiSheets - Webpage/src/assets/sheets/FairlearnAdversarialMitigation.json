{
    "filename": "FairlearnAdversarial",
    "metadata":
    {
        "name": "Adversarial Mitigation",
        "authors": "Fairlearn",
        "version": "0.11.0",
        "license": "MIT",
        "source_paper": "\\textit{Mitigating Unwanted Biases with Adversarial Learning} \\cite{Zhang2018}"
    },
    "description":
    {
        "method_type": ["Adversarial Learning"],
        "task": ["Classification", "Regression"],
        "compatible_data": ["Tabular Datasets"],
        "method_description":"The predictor network is constructed to solve the underlying supervised learning task, without considering fairness, by minimizing the predictor loss. However, to improve fairness, we do not only minimize the predictor loss, but we also want to decrease the adversary’s ability to predict the sensitive features from the predictor’s predictions (when implementing demographic parity), or jointly from the predictor’s predictions and true labels (when implementing equalized odds)."
    },
    "pipeline":
    {
        "pipeline_location": "In-Processing",
        "compatible_model": ["Neural Networks"],
        "model_description": "The neural network models cannot have an activation function or discrete prediction function on the final layer."
    },
    "fairness":
    {
        "composition_features": ["Categorical Attributes","Numerical Attribute"],
        "fairness_guarantee": "Tunable Fairness Strength",
        "fairness_type_defs": [{
            "fairness_type": "Group Fairness",
            "fairness_definitions": ["Demographic Parity", "Equalized Odds"]}],
        "fairness_description": "Fairness is achieved by minimizing the adversary's ability to predict the sensitive attribute. This is trade-off with the regular loss of the system, the strength of the fairness intervention can be tuned through a hyperparameter."
    },
    "implementation":
    {
        "packages": [{"programming_language": "Python",
                      "related_packages": ["scikit-learn", "PyTorch", "Tensorflow Keras"] }],
        "description": "The data needs to be provided as a 2D-arraylike of floats. The labels can be binary, categorical or float."
    },
    "use_cases": 
    {
        "cases": ["Adult \\cite{adult_2}"],
        "description": ""
    },
    "bibliography": [" @inproceedings{Zhang2018, author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret}, title = {Mitigating Unwanted Biases with Adversarial Learning}, year = {2018}, isbn = {9781450360128}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3278721.3278779}, doi = {10.1145/3278721.3278779}, booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society}, pages = {335–340}, numpages = {6}, keywords = {adversarial learning, debiasing, multi-task learning, unbiasing}, location = {New Orleans, LA, USA}, series = {AIES '18}}",
                    "@misc{adult_2, author = {Becker, Barry and Kohavi, Ronny}, title = {{Adult}}, year = {1996}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5XW20}}"]
}