const e="ErrorParity",i={name:"Error-parity",authors:"André Cruz and Moritz Hardt",version:"0.3.11",license:"MIT License",source_paper:"\\textit{Unprocessing Seven Years of Algorithmic Fairness}. \\cite{cruz2024unprocessing}"},a={method_type:["Thresholding"],task:["Binary Classification","Hard Labels"],compatible_data:["Dataset Independent"],method_description:"Error-parity sets groupspecific acceptance thresholds so as to \\textbf{minimize risk while achieving an equality in error rates} across a desired set of groups. It is both simple and computationally efficient. Error-parity achieves exact error rate equality, unlike many preprocessing and inprocessing, which achieve some relaxation of the constraint. The method uses the output scores and returns hard prediction labels. "},t={pipeline_location:"Post-Processing",compatible_model:["Probabilistic Classifier"],model_description:"Error-parity is compatible with any underlying learner that can produce scores of predicted probabilities. "},n={composition_features:["Categorical Attributes"],fairness_guarantee:"Fairness Guaranteed",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity","Equalized Odds","Equal Opportunity","Predictive Equality"]}],fairness_description:""},r={packages:[{programming_language:"Python 3.8-3.12",related_packages:["scikit-learn","fairlearn"]}],description:"The implementation requires a trained score predictor that takes in samples, X, in shape (num\\_samples, num\\_features), and outputs real-valued scores, R, in shape (num\\_samples,) as the model that feeds into error-parity. "},s={cases:["Synthetic","Adult","Folktables"],description:""},o=["@inproceedings{cruz2024unprocessing, title={Unprocessing Seven Years of Algorithmic Fairness}, author={Andr{\\'e} Cruz and Moritz Hardt}, booktitle={The Twelfth International Conference on Learning Representations}, year={2024}, url={https://openreview.net/forum?id=jr03SfWsBS}}"],c={filename:e,metadata:i,description:a,pipeline:t,fairness:n,implementation:r,use_cases:s,bibliography:o},l="AIF360AdversarialDebiasing",d={name:"Adversarial Debiasing",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"Mitigating Unwanted Biases with Adversarial Learning",source_paper_citation:["Zhang2018"]},p={method_type:["Adversarial Learning","Regularization"],task:["Binary Classification"],compatible_data:["Tabular Datasets"],method_description:"Adversarial debiasing is an in-processing technique that learns a classifier to maximize prediction accuracy and simultaneously reduce an adversary’s ability to determine the protected attribute from the predictions. This approach leads to a fair classifier as the predictions cannot carry any group discrimination information that the adversary can exploit."},h={pipeline_location:"In-Processing",compatible_model:["Neural Networks"],model_description:"The method is compatible with any gradient-based learning model, however the current implementation only allows for a neural network with one hidden layer."},m={composition_features:["Binary Attribute"],fairness_guarantee:"Tunable Fairness Strength",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity"]}],fairness_description:"The adversarial model tries to predict the sensitive attributes from the prediction of the model. The method tries to prevent this from happening, effectively making the prediction independent of the sensitive attribute, which is equivalent to Demographic Parity."},u={packages:[{programming_language:"Python",related_packages:[]}],description:"The model runs a Tensorflow neural network with one hidden layer. Input data has to be formatted as an AIF360 dataset."},g={cases:["Adult"],description:"Additional experiments were done in Zhang et al. on the Google analogy data set in Mikolov et al. .",description_citations:["Zhang2018","Mikolov_Sutskever_Chen_Corrado_Dean_2013"]},f=["@misc{adult_2, author = {Becker, Barry and Kohavi, Ronny}, title = {Adult}, year = {1996}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5XW20}}"," @inproceedings{Mikolov_Sutskever_Chen_Corrado_Dean_2013, title={Distributed Representations of Words and Phrases and their Compositionality}, volume={26}, url={https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff}, year={2013} }"," @inproceedings{Zhang2018, author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret}, title = {Mitigating Unwanted Biases with Adversarial Learning}, year = {2018}, isbn = {9781450360128}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3278721.3278779}, doi = {10.1145/3278721.3278779}, booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society}, pages = {335–340}, numpages = {6}, keywords = {unbiasing, multi-task learning, debiasing, adversarial learning}, location = {New Orleans, LA, USA}, series = {AIES 18}}"],y={filename:l,metadata:d,description:p,pipeline:h,fairness:m,implementation:u,use_cases:g,bibliography:f},_="AIF360CalibratedEqOddsPostprocessing",b={name:"Calibrated Equalized Odds Post-processing",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"\\textit{On fairness and calibration} \\cite{pleiss2017fairness}"},A={method_type:["Calibration"],task:["Binary Classification"],compatible_data:["Dataset Independent"],method_description:"Equalized Odds Post-Processing is a post-processing technique that seeks to attain scores that optimize (a relaxation of) equalized odds while the classifier remains calibrated. This is achieved by changing the probabilities of certain samples to the base rate of the group rather than the classifiers prediction."},k={pipeline_location:"Post-Processing",compatible_model:["Calibrated Classifier"],model_description:"Calibrated Equalized Odds Post-processing is compatible with any underlying learner that outputs a calibrated classifier with regards to the sensitive groups. "},w={composition_features:["Binary Attribute"],fairness_guarantee:"No Fairness Guarantee",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Equal Opportunity","Predictive Equality","Calibration"]}],fairness_description:"The method aims to satisfy either the chosen fairness measure, while maintaining calibration between satisfiers. This is achieved by sacrificing performance for one group. Experimental results show no guarantee on the performance of the method."},v={packages:[{programming_language:"Python",related_packages:["scikit-learn"]}],description:"While the concept of the method is compatible with any calibrated classifier, this implementation uses a set model type. The dataset must be passed in the AIF360 used format."},C={cases:["Adult \\cite{adult_2}","German \\cite{german_credit_data}","Compas \\cite{Larson_Angwin_Kirchner}"],description:"The method does not seem to improve the fairness of a model when experimented on Adult. "},I=["@article{pleiss2017fairness, title={On fairness and calibration}, author={Pleiss, Geoff and Raghavan, Manish and Wu, Felix and Kleinberg, Jon and Weinberger, Kilian Q}, journal={Advances in neural information processing systems}, volume={30}, year={2017}}","@misc{adult_2, author = {Becker, Barry and Kohavi, Ronny}, title = {{Adult}}, year = {1996}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5XW20}}","@misc{Larson_Angwin_Kirchner, title={How We Analyzed the COMPAS Recidivism Algorithm}, url={https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}, journal={ProPublica}, author={Larson, Jeff and Angwin, Julia and Kirchner, Lauren}, language={en} }","@misc{german_credit_data, author = {Hofmann, Hans}, title = {{Statlog (German Credit Data)}}, year = {1994}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5NC77}}"],P={filename:_,metadata:b,description:A,pipeline:k,fairness:w,implementation:v,use_cases:C,bibliography:I},F="AIF360DeterministicRerankingConservative",R={name:"Deterministic Reranking - Conservative",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"Fairness-Aware Ranking in Search & Recommendation Systems with Application to LinkedIn Talent Search \\cite{geyik2019fairness}"},D={method_type:["Re-ranking"],task:["Ranking"],compatible_data:["Dataset Independent"],method_description:"If there are any groups for which the minimum representation constraint is violated, choose the element with the highest score among those groups. Otherwise, among groups that do not violate the maximum constraint, pick the group that minimizes $ \\frac{\\lceil p_a * k \\rceil}{p_a} $. From this group, choose the element with the highest score."},$={pipeline_location:"Post-Processing",compatible_model:["Score-based ranker"],model_description:"Deterministic Reranking takes as input a list of output scores (for a each member of a list of candidates) of a ranker. Candidates are by convention assumed to be ordered by decreasing score by the initial ranker."},S={composition_features:["Categorical Attributes"],fairness_guarantee:"Fairness Guaranteed",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Adjusted Demographic Parity"]}],fairness_description:"The Deterministic Reranker allows to set a desired proportion $p_a$ per demographic group $ a $. Given k samples, the algorithm ensures that the number of selected candidates from each group $n_a$ satisfies $ \\lfloor p_a * k \\rfloor \\leq n_a \\leq \\lceil p_a * k \\rceil $."},T={packages:[{programming_language:"Python",related_packages:["scikit-learn"]}],description:"The dataset must be passed in the AIF360 used format."},L={cases:["Synthetic Dataset"],description:"The original paper also tested the method on a proprietary dataset."},M=[" @inproceedings{geyik2019fairness, title={Fairness-aware ranking in search \\& recommendation systems with application to linkedin talent search}, author={Geyik, Sahin Cem and Ambler, Stuart and Kenthapadi, Krishnaram}, booktitle={Proceedings of the 25th acm sigkdd international conference on knowledge discovery \\& data mining}, pages={2221--2231}, year={2019}"],O={filename:F,metadata:R,description:D,pipeline:$,fairness:S,implementation:T,use_cases:L,bibliography:M},z="AIF360DeterministicRerankingConstrained",B={name:"Deterministic Reranking - Constrained",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"Fairness-Aware Ranking in Search & Recommendation Systems with Application to LinkedIn Talent Search \\cite{geyik2019fairness}"},E={method_type:["Re-ranking"],task:["Ranking"],compatible_data:["Dataset Independent"],method_description:"Starting with 0, increase the value of k until the minimum representation constraint is increased for at least one group. If there are more than one such groups, order them according to the descending score of their highest-scoring candidates not yet in the ranking. For each group in the list above: 1. Insert the next candidate from the group to the next empty index in the ranking Swap the candidate towards earlier indices until: Either the score of the candidate in the earlier index is larger, or, 2. Swapping will violate the minimum condition for the group of the candidate in the earlier index."},G={pipeline_location:"Post-Processing",compatible_model:["Score-based ranker"],model_description:"Deterministic Reranking takes as input a list of output scores (for a each member of a list of candidates) of a ranker. Candidates are by convention assumed to be ordered by decreasing score by the initial ranker."},x={composition_features:["Categorical Attributes"],fairness_guarantee:"Fairness Guaranteed",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Adjusted Demographic Parity"]}],fairness_description:"The Deterministic Reranker allows to set a desired proportion $p_a$ per demographic group $ a $. Given k samples, the algorithm ensures that the number of selected candidates from each group $n_a$ satisfies."},K={packages:[{programming_language:"Python",related_packages:["scikit-learn"]}],description:"The dataset must be passed in the AIF360 used format."},q={cases:["Synthetic Dataset"],description:"The original paper also tested the method on a proprietary dataset."},j=[" @inproceedings{geyik2019fairness, title={Fairness-aware ranking in search \\& recommendation systems with application to linkedin talent search}, author={Geyik, Sahin Cem and Ambler, Stuart and Kenthapadi, Krishnaram}, booktitle={Proceedings of the 25th acm sigkdd international conference on knowledge discovery \\& data mining}, pages={2221--2231}, year={2019}"],N={filename:z,metadata:B,description:E,pipeline:G,fairness:x,implementation:K,use_cases:q,bibliography:j},H="AIF360DeterministicRerankingGreedy",J={name:"Deterministic Reranking - Greedy",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"\\textit{Fairness-Aware Ranking in Search \\& Recommendation Systems with Application to LinkedIn Talent Search} \\cite{geyik2019fairness}"},U={method_type:["Re-ranking"],task:["Ranking"],compatible_data:["Dataset Independent"],method_description:"The attribute value with the highest next score among those that have not yet met their desired proportion is chosen. However, if there are any attribute values for which the desired proportion is about to be violated, the one with the highest next score among them is chosen."},W={pipeline_location:"Post-Processing",compatible_model:["Score-based ranker"],model_description:"Deterministic Reranking takes as input a list of output scores (for a each member of a list of candidates) of a ranker. Candidates are by convention assumed to be ordered by decreasing score by the initial ranker."},X={composition_features:["Categorical Attributes"],fairness_guarantee:"Fairness Guaranteed",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Adjusted Demographic Parity"]}],fairness_description:"The Deterministic Reranker allows to set a desired proportion $p_a$ per demographic group $ a $. Given k samples, the algorithm ensures that the number of selected candidates from each group $n_a$ satisfies $ \\lfloor p_a * k \\rfloor \\leq n_a \\leq \\lceil p_a * k \\rceil $."},Y={packages:[{programming_language:"Python",related_packages:["scikit-learn"]}],description:"The dataset must be passed in the AIF360 used format."},V={cases:["Synthetic Dataset"],description:"The original paper also tested the method on a proprietary dataset."},Z=[" @inproceedings{geyik2019fairness, title={Fairness-aware ranking in search \\& recommendation systems with application to linkedin talent search}, author={Geyik, Sahin Cem and Ambler, Stuart and Kenthapadi, Krishnaram}, booktitle={Proceedings of the 25th acm sigkdd international conference on knowledge discovery \\& data mining}, pages={2221--2231}, year={2019}"],Q={filename:H,metadata:J,description:U,pipeline:W,fairness:X,implementation:Y,use_cases:V,bibliography:Z},ee="AIF360DeterministicRerankingRelaxed",ie={name:"Deterministic Reranking - Relaxed",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"\\textit{Fairness-Aware Ranking in Search \\& Recommendation Systems with Application to LinkedIn Talent Search} \\cite{geyik2019fairness}"},ae={method_type:["Re-ranking"],task:["Ranking"],compatible_data:["Dataset Independent"],method_description:"If there are any groups for which the minimum representation constraint is violated, choose the element with the highest score among those groups. Otherwise, among groups that do not violate the maximum constraint, pick the group that minimizes $ \\lceil \\frac{\\lceil p_a * k \\rceil}{p_a} \\rceil $. From this group, choose the element with the highest score."},te={pipeline_location:"Post-Processing",compatible_model:["Score-based ranker"],model_description:"Deterministic Reranking takes as input a list of output scores (for a each member of a list of candidates) of a ranker. Candidates are by convention assumed to be ordered by decreasing score by the initial ranker."},ne={composition_features:["Categorical Attributes"],fairness_guarantee:"Fairness Guaranteed",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Adjusted Demographic Parity"]}],fairness_description:"The Deterministic Reranker allows to set a desired proportion $p_a$ per demographic group $ a $. Given k samples, the algorithm ensures that the number of selected candidates from each group $n_a$ satisfies $ \\lfloor p_a * k \\rfloor \\leq n_a \\leq \\lceil p_a * k \\rceil $."},re={packages:[{programming_language:"Python",related_packages:["scikit-learn"]}],description:"The dataset must be passed in the AIF360 used format."},se={cases:["Synthetic Dataset"],description:"The original paper also tested the method on a proprietary dataset."},oe=[" @inproceedings{geyik2019fairness, title={Fairness-aware ranking in search \\& recommendation systems with application to linkedin talent search}, author={Geyik, Sahin Cem and Ambler, Stuart and Kenthapadi, Krishnaram}, booktitle={Proceedings of the 25th acm sigkdd international conference on knowledge discovery \\& data mining}, pages={2221--2231}, year={2019}"],ce={filename:ee,metadata:ie,description:ae,pipeline:te,fairness:ne,implementation:re,use_cases:se,bibliography:oe},le="AIF360DisparateImpactRemover",de={name:"Disparate Impact Remover",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"\\textit{Certifying and Removing Disparate Impact} \\cite{Feldman2015}"},pe={method_type:["Transformation"],task:["Binary Classification"],compatible_data:["Tabular Datasets"],method_description:"Disparate Impact Remover changes the values of the labels while preserving ranking."},he={pipeline_location:"Pre-Processing",compatible_model:["Model Independent"],model_description:"The original paper tested with Logistic Regression, Support Vector Machines, and Gaussian Naïve Bayes models. "},me={composition_features:["Binary Attribute"],fairness_guarantee:"Tunable Fairness Strength",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Balanced Error Rate Parity"]}],fairness_description:"The method aims to have the balanced error rate higher than a specific threshold. This follows"},ue={packages:[{programming_language:"Python",related_packages:["scikit-learn"]}],description:"The implementation of the method allows only binary attributes, the original paper states that the method can be applied for parallel attributes. Input data has to be formatted as an AIF360 dataset"},ge={cases:["Adult \\cite{adult_2}"],description:"Experiments were conducted on the Ricci Data \\cite{Ricci}, German credit \\cite{german_credit_data}, and Adult income \\cite{adult_2} in \\citet{Feldman2015}."},fe=["@inproceedings{Feldman2015, author = {Feldman, Michael and Friedler, Sorelle A. and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh}, title = {Certifying and Removing Disparate Impact}, year = {2015}, isbn = {9781450336642}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2783258.2783311}, doi = {10.1145/2783258.2783311}, booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, pages = {259–268}, numpages = {10}, keywords = {disparate impact, fairness, machine learning}, location = {Sydney, NSW, Australia}, series = {KDD '15} }","@misc{Ricci, title={Ricci v. DeStefano, 557 U.S. 557}, year={2009}}","@misc{adult_2, author = {Becker, Barry and Kohavi, Ronny}, title = {{Adult}}, year = {1996}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5XW20}}","@misc{german_credit_data, author = {Hofmann, Hans}, title = {Statlog (German Credit Data)}, year = {1994}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5NC77}}"],ye={filename:le,metadata:de,description:pe,pipeline:he,fairness:me,implementation:ue,use_cases:ge,bibliography:fe},_e="AIF360ExponentiatedGradientReduction",be={name:"Grid Search Reduction",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"\\textit{A Reductions Approach to Fair Classification} \\cite{agarwal2018reductions}"},Ae={method_type:["Constraint Optimization"],task:["Binary Classification"],compatible_data:["Dataset Independent"],method_description:"Exponentiated gradient reduction is an in-processing technique that reduces fair classification to a sequence of cost-sensitive classification problems, returning a randomized classifier with the lowest empirical error subject to fair classification constraints."},ke={pipeline_location:"In-Processing",compatible_model:["Model Independent"],model_description:"Grid Search Reduction is compatible with any underlying learner that can produce 0-1 predictions."},we={composition_features:["Categorical Attributes"],fairness_guarantee:"Fairness Guaranteed",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity","Equalized Odds","Equal Opportunity","Predictive Equality"]}],fairness_description:"A difference bound is passed to the system which details the allowed fairness violation. The difference between two groups will be at most twice the value of this difference bound hyperparameter."},ve={packages:[{programming_language:"Python",related_packages:["scikit-learn"]}],description:"Input data has to be formatted as an AIF360 dataset. Users are required to provide a classifier or regression model satisfying scikit-learn conventions for the 'fit' and 'predict' methods (wth 0-1 outputs for 'predict')."},Ce={cases:["Adult \\cite{adult_2}"],description:""},Ie=["@inproceedings{agarwal2018reductions, title={A reductions approach to fair classification}, author={Agarwal, Alekh and Beygelzimer, Alina and Dud{\\'\\i}k, Miroslav and Langford, John and Wallach, Hanna}, booktitle={International conference on machine learning}, pages={60--69}, year={2018}, organization={PMLR}}","@inproceedings{freund1996game, title={Game theory, on-line prediction and boosting}, author={Freund, Yoav and Schapire, Robert E}, booktitle={Proceedings of the ninth annual conference on Computational learning theory}, pages={325--332}, year={1996}}","@misc{adult_2, author = {Becker, Barry and Kohavi, Ronny}, title = {{Adult}}, year = {1996}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5XW20}}"],Pe={filename:_e,metadata:be,description:Ae,pipeline:ke,fairness:we,implementation:ve,use_cases:Ce,bibliography:Ie},Fe="AIF360GerryFairClassifier",Re={name:"GerryFair Classifier",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"\\textit{Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness} \\cite{Kearns2018} and \\textit{An Empirical Study of Rich Subgroup Fairness for Machine Learning} \\cite{Kearns2019}"},De={method_type:["Subgroup Analysis","Calibration"],task:["Binary Classification"],compatible_data:["Tabular Datasets"],method_description:"The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). The Learner is trying to minimize the sum of the prediction error and a fairness penalty term (given by the Lagrangian), and the Auditor is trying to penalize the fairness violation of the Learner by first identifying the subgroup with the greatest fairness violation and putting all the weight on the dual variable corresponding to this subgroup. "},$e={pipeline_location:"In-Processing",compatible_model:["Linear Regression","Support Vector Machines","Decision Trees","Kernel Regression"],model_description:""},Se={composition_features:["Categorical Attributes"],fairness_guarantee:"Fairness Guaranteed",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity","Equal Opportunity","Predictive Equality"]}],fairness_description:"The method stops when the difference in the subgroup's metric with the overall value of the metric weighted with their proportion is smaller than a chosen hyperparameter gamma. "},Te={packages:[{programming_language:"Python",related_packages:["scikit-learn"]}],description:""},Le={cases:["Adult \\cite{adult_2}"],description:""},Me=["@InProceedings{Kearns2018, title = {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness}, author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven}, booktitle = {Proceedings of the 35th International Conference on Machine Learning}, pages = {2564--2572}, year = {2018}, editor = {Dy, Jennifer and Krause, Andreas}, volume = {80}, series = {Proceedings of Machine Learning Research},month = {10--15 Jul}, publisher = {PMLR},pdf = {http://proceedings.mlr.press/v80/kearns18a/kearns18a.pdf},url = {https://proceedings.mlr.press/v80/kearns18a.html}}","@inproceedings{Kearns2019, author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven}, title = {An Empirical Study of Rich Subgroup Fairness for Machine Learning}, year = {2019}, isbn = {9781450361255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3287560.3287592}, doi = {10.1145/3287560.3287592}, booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency}, pages = {100–109}, numpages = {10}, keywords = {Subgroup Fairness, Fairness Auditing, Fair Classification, Algorithmic Bias}, location = {Atlanta, GA, USA}, series = {FAT* '19}}","@misc{adult_2, author = {Becker, Barry and Kohavi, Ronny}, title = {{Adult}}, year = {1996}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5XW20}}"],Oe={filename:Fe,metadata:Re,description:De,pipeline:$e,fairness:Se,implementation:Te,use_cases:Le,bibliography:Me},ze="AIF360GridSearchReduction",Be={name:"Grid Search Reduction",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"\\textit{A Reductions Approach to Fair Classification}, \\textit{Fair Regression: Quantitative Definitions and Reduction-based Algorithms} \\cite{agarwal2018reductions, agarwal2019fair}"},Ee={method_type:["Constraint Optimization"],task:["Binary Classification","Hard Labels","Regression"],compatible_data:["Dataset Independent"],method_description:"Grid Search Reduction reduces the task to a sequence of cost-sensitive classification or regression problems, returning the deterministic classifier/regressor with the lowest empirical error subject to fairness constraints."},Ge={pipeline_location:"In-Processing",compatible_model:["Model Independent"],model_description:"Grid Search Reduction is compatible with any underlying learner that can produce 0-1 predictions in the classification case, and any regression model in the regression case."},xe={composition_features:["Categorical Attributes"],fairness_guarantee:"No Fairness Guarantee",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity","Equalized Odds","Error Rate Parity","True and False Positive Rate Parity","Bounded Group Loss","Moment based fairness definitions"]}],fairness_description:"The approach applies to fairness definitions covered by \\textit{fairlearn.reductions.Moment} (including demographic parity, equalized odds, error rate parity, true and false positive rate parity for classification; and bounded group loss for regression)."},Ke={packages:[{programming_language:"Python",related_packages:["scikit-learn","aif360"]}],description:"Input data has to be formatted as an AIF360 dataset. Users are required to provide a classifier or regression model satisfying scikit-learn conventions for the 'fit' and 'predict' methods. In the classification case, 'predict' should yield 0-1 outputs."},qe={cases:["Adult \\cite{adult_2}","Law School \\cite{law_school}"],description:""},je=["@inproceedings{agarwal2018reductions, title={A reductions approach to fair classification}, author={Agarwal, Alekh and Beygelzimer, Alina and Dud{\\'\\i}k, Miroslav and Langford, John and Wallach, Hanna}, booktitle={International conference on machine learning}, pages={60--69}, year={2018}, organization={PMLR}}","@inproceedings{agarwal2019fair, title={Fair regression: Quantitative definitions and reduction-based algorithms}, author={Agarwal, Alekh and Dud{\\'\\i}k, Miroslav and Wu, Zhiwei Steven}, booktitle={International Conference on Machine Learning}, pages={120--129}, year={2019}, organization={PMLR}}","@misc{adult_2, author = {Becker, Barry and Kohavi, Ronny}, title = {{Adult}}, year = {1996}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5XW20}}","@misc{law_school, author = {Wightman, Linda}, title={LSAC National Longitudinal Bar Passage Study. LSAC Research Report Series}, year={1998}"],Ne={filename:ze,metadata:Be,description:Ee,pipeline:Ge,fairness:xe,implementation:Ke,use_cases:qe,bibliography:je},He="AIF360LFR",Je={name:"LFR",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"\\textit{Learning Fair Representations} \\cite{Zemel13}"},Ue={method_type:["Adversarial Learning"],task:["Binary Classification"],compatible_data:["Tabular Datasets"],method_description:"LFR maps each individual, represented as a data point in a given input space, to a probability distribution in a new representation space. The aim of this new representation is to lose any information that can identify whether the person belongs to the protected subgroup, while retaining as much other information as possible."},We={pipeline_location:"Pre-Processing",compatible_model:["Model Independent"],model_description:""},Xe={composition_features:["Binary Attribute"],fairness_guarantee:"Tunable Fairness Strength",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity"]},{fairness_type:"Individual Fairness",fairness_definitions:[]}],fairness_description:"The method minimizes an objection which is a combination of demographic parity, information loss in the representation, and accuracy loss of the prediction label. The strength for each of these elements can be tuned."},Ye={packages:[{programming_language:"Python",related_packages:[]}],description:"Input data has to be formatted as an AIF360 dataset."},Ve={cases:["Adult \\cite{adult_2}"],description:"Experiments were conducted on Adult \\cite{adult_2}, German credit \\cite{german_credit_data}, and Heritage Health Prize milestone 1 challenge \\cite{Brierley_Vogel_Axelrod} in \\citet{Zemel13}."},Ze=["@InProceedings{Zemel13, title = {Learning Fair Representations}, author = {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia}, booktitle = {Proceedings of the 30th International Conference on Machine Learning}, pages = {325--333}, year = {2013}, editor = {Dasgupta, Sanjoy and McAllester, David}, volume = {28}, number = {3},series = {Proceedings of Machine Learning Research},address = {Atlanta, Georgia, USA},month = {17--19 Jun},publisher = {PMLR},pdf = {http://proceedings.mlr.press/v28/zemel13.pdf},url = {https://proceedings.mlr.press/v28/zemel13.html}}","@misc{adult_2, author = {Becker, Barry and Kohavi, Ronny}, title = {{Adult}}, year = {1996}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5XW20}}","@misc{german_credit_data, author = {Hofmann, Hans}, title = {{Statlog (German Credit Data)}}, year = {1994}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5NC77}}"," @article{Brierley_Vogel_Axelrod, title={Heritage Provider Network Health Prize Round 1 Milestone Prize How We Did It – Team ‘Market Makers’}, author={Brierley, Phil and Vogel, David and Axelrod, Randy}, language={en} }"],Qe={filename:He,metadata:Je,description:Ue,pipeline:We,fairness:Xe,implementation:Ye,use_cases:Ve,bibliography:Ze},ei="AIF360MetaFairClassifier",ii={name:"MetaFair Classifier",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"\\textit{Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees} \\cite{Celis2019}"},ai={method_type:["Constraint Optimization"],task:["Binary Classification"],compatible_data:["Tabular Datasets"],method_description:"The MetaFair Classifier reduces classification with linear-fractional constraints to solving a small number of linear classification problems for carefully chosen parameters."},ti={pipeline_location:"In-Processing",compatible_model:["Bayesian model"],model_description:"The method has an accompanying model implementation used for predictions."},ni={composition_features:["Categorical Attributes"],fairness_guarantee:"Tunable Fairness Strength",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity","Predictive Parity"]}],fairness_description:"The original model can satisfy any linear-fractional statistic, however only two are implemented here. The difference in fairness measure is noted as a fraction of the measures."},ri={packages:[{programming_language:"Python",related_packages:["scikit-learn"]}],description:"Input data has to be formatted as an AIF360 dataset."},si={cases:["Adult \\cite{adult_2}"],description:"Experiments on Adult \\cite{adult_2}, German credit \\cite{german_credit_data} and COMPAS \\cite{Larson_Angwin_Kirchner} were conducted in the original paper of \\citet{Celis2019}."},oi=["@inproceedings{Celis2019, author = {Celis, L. Elisa and Huang, Lingxiao and Keswani, Vijay and Vishnoi, Nisheeth K.}, title = {Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees}, year = {2019}, isbn = {9781450361255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3287560.3287586}, doi = {10.1145/3287560.3287586}, booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency}, pages = {319–328}, numpages = {10}, keywords = {Algorithmic Fairness, Classification}, location = {Atlanta, GA, USA}, series = {FAT* '19} }","@misc{adult_2, author = {Becker, Barry and Kohavi, Ronny}, title = {{Adult}}, year = {1996}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5XW20}}","@misc{Larson_Angwin_Kirchner, title={How We Analyzed the COMPAS Recidivism Algorithm}, url={https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}, journal={ProPublica}, author={Larson, Jeff and Angwin, Julia and Kirchner, Lauren}, language={en} }","@misc{german_credit_data, author = {Hofmann, Hans}, title = {{Statlog (German Credit Data)}}, year = {1994}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5NC77}}"],ci={filename:ei,metadata:ii,description:ai,pipeline:ti,fairness:ni,implementation:ri,use_cases:si,bibliography:oi},li="AIF360OptimPreproc",di={name:"Optimized Preprocessing ",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"\\textit{Optimized Pre-Processing for Discrimination Prevention} \\cite{Calmon_2017}"},pi={method_type:["Transformation"],task:["Binary Classification"],compatible_data:["Tabular Datasets"],method_description:"Optimized Preprocessing is a preprocessing technique that learns a probabilistic transformation that edits the features and labels in the data with group fairness, individual distortion, and data fidelity constraints and objectives."},hi={pipeline_location:"Pre-Processing",compatible_model:["Model Independent"],model_description:"The original paper conducted experiments with Logistic Regression and Random Forest. \\cite{Calmon_2017}"},mi={composition_features:["Categorical Attributes"],fairness_guarantee:"Fairness Guaranteed",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity"]}],fairness_description:"The fairness guarantee is provided on the dataset, not the model outcome. The method ensures that the fraction between positive rates is smaller than a hyperparameter epsilon, while keeping the distortion on the features under a given threshold."},ui={packages:[{programming_language:"Python",related_packages:["scikit-learn"]}],description:"The implementation does not use the initialized privileged and unprivileged groups in the initialization but the info on demographic attributes in the pre-processing of the dataset. The Optimizer dict contains the tunable parameters of the function."},gi={cases:["Adult \\cite{adult_2}","German \\cite{german_credit_data}","COMPAS \\cite{Larson_Angwin_Kirchner}"],description:""},fi=[" @inproceedings{Calmon_2017, title={Optimized Pre-Processing for Discrimination Prevention}, volume={30}, url={https://papers.nips.cc/paper_files/paper/2017/hash/9a49a25d845a483fae4be7e341368e36-Abstract.html}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R}, year={2017} }","@misc{adult_2, author = {Becker, Barry and Kohavi, Ronny}, title = {{Adult}}, year = {1996}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5XW20}}","@misc{Larson_Angwin_Kirchner, title={How We Analyzed the COMPAS Recidivism Algorithm}, url={https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}, journal={ProPublica}, author={Larson, Jeff and Angwin, Julia and Kirchner, Lauren}, language={en} }","@misc{german_credit_data, author = {Hofmann, Hans}, title = {{Statlog (German Credit Data)}}, year = {1994}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5NC77}}"],yi={filename:li,metadata:di,description:pi,pipeline:hi,fairness:mi,implementation:ui,use_cases:gi,bibliography:fi},_i="AIF360PrejudiceRemover",bi={name:"Prejudice Remover",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"\\textit{Fairness-aware classifier with prejudice remover regularizer} \\cite{kamishima2012fairness}"},Ai={method_type:["Regularization"],task:["Binary Classification"],compatible_data:["Dataset Independent"],method_description:"The Prejudice Remover adds a penalty to the learning objective that aims to reduce 'indirect prejudice' - the mutual information between the protected attribute and the model's predictions."},ki={pipeline_location:"In-Processing",compatible_model:["Logistic Regression"],model_description:"This bias mitigation method is currently only implemented for logistic regression."},wi={composition_features:["Binary Attribute"],fairness_guarantee:"Tunable Fairness Strength",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity"]}],fairness_description:"The penalization terms aims to reduce 'indirect prejudice', minimizing mutual information between model predictions and sensitive attributes."},vi={packages:[{programming_language:"Python",related_packages:[]}],description:"Input data has to be formatted as an AIF360 dataset. "},Ci={cases:[],description:"Experiments on Adult \\cite{adult_2} were conducted in \\citet{kamishima2012fairness}, it is not certain if this is the same implementation."},Ii=["@inproceedings{kamishima2012fairness, title={Fairness-aware classifier with prejudice remover regularizer}, author={Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun}, booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, Part II 23}, pages={35--50}, year={2012}, organization={Springer}}","@misc{adult_2, author = {Becker, Barry and Kohavi, Ronny}, title = {{Adult}}, year = {1996}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5XW20}}"],Pi={filename:_i,metadata:bi,description:Ai,pipeline:ki,fairness:wi,implementation:vi,use_cases:Ci,bibliography:Ii},Fi="AIF360RejectOptionClassification",Ri={name:"Reject Option Classification",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"\\textit{Decision theory for discrimination-aware classification} \\cite{kamiran2012decision}"},Di={method_type:["Thresholding"],task:["Binary Classification","Hard Labels"],compatible_data:["Dataset Independent"],method_description:"Reject option classification is a postprocessing technique that gives favorable outcomes to unpriviliged groups and unfavorable outcomes to priviliged groups in a confidence band around the decision boundary with the highest uncertainty. This is done by flipping the labels for the samples of these groups close to the decision boundary."},$i={pipeline_location:"Post-Processing",compatible_model:["Probabilistic Classifier"],model_description:"Reject Option Classification is compatible with any underlying learner that can produce scores of predicted probabilities."},Si={composition_features:["Binary Attribute"],fairness_guarantee:"No Fairness Guarantee",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity","Equalized Odds","Equal Opportunity"]}],fairness_description:"The critical region size is determined based on optimizing the chosen fairness guarantee."},Ti={packages:[{programming_language:"Python",related_packages:["scikit-learn"]}],description:"Input data has to be formatted as an AIF360 dataset."},Li={cases:["Adult \\cite{adult_2}","German \\cite{german_credit_data}","Compas \\cite{Larson_Angwin_Kirchner}"],description:""},Mi=["@inproceedings{kamiran2012decision, title={Decision theory for discrimination-aware classification}, author={Kamiran, Faisal and Karim, Asim and Zhang, Xiangliang}, booktitle={2012 IEEE 12th international conference on data mining}, pages={924--929}, year={2012}, organization={IEEE}}","@misc{adult_2, author = {Becker, Barry and Kohavi, Ronny}, title = {{Adult}}, year = {1996}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5XW20}}","@misc{Larson_Angwin_Kirchner, title={How We Analyzed the COMPAS Recidivism Algorithm}, url={https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}, journal={ProPublica}, author={Larson, Jeff and Angwin, Julia and Kirchner, Lauren}, language={en} }","@misc{german_credit_data, author = {Hofmann, Hans}, title = {{Statlog (German Credit Data)}}, year = {1994}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5NC77}}"],Oi={filename:Fi,metadata:Ri,description:Di,pipeline:$i,fairness:Si,implementation:Ti,use_cases:Li,bibliography:Mi},zi="AIF360Reweighing",Bi={name:"Reweighing",authors:"AIF360",version:"0.6.1",license:"Apache 2.0",source_paper:"\\textit{Data Preprocessing Techniques for Classification without Discrimination} \\cite{Kamiran_Calders_2012}"},Ei={method_type:["Reweighing"],task:["Binary Classification"],compatible_data:["Tabular Datasets"],method_description:"The tuples in the training dataset are assigned weights. By carefully choosing the weights, the training dataset can be made discrimination-free w.r.t. S without having to change any of the labels. The weights on the tuples can be used directly in any method based on frequency counts."},Gi={pipeline_location:"Pre-Processing",compatible_model:["Model which incorporates weigths"],model_description:"The method is effectively model independent, however for the method to be useful then the model must be able to incorporate the weights produced by the method."},xi={composition_features:["Binary Attribute"],fairness_guarantee:"No Fairness Guarantee",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity"]}],fairness_description:"The reweighing factor is the fraction of the expected frequency to the observed frequency wrt the combination of sensitive attribute and its class."},Ki={packages:[{programming_language:"Python",related_packages:["scikit-learn"]}],description:"Input data has to be formatted as an AIF360 dataset."},qi={cases:["Adult \\cite{adult_2}, German \\cite{Larson_Angwin_Kirchner}, COMPAS \\cite{german_credit_data}"],description:""},ji=[" @article{Kamiran_Calders_2012, title={Data preprocessing techniques for classification without discrimination}, volume={33}, ISSN={0219-3116}, DOI={10.1007/s10115-011-0463-8}, number={1}, journal={Knowledge and Information Systems}, author={Kamiran, Faisal and Calders, Toon}, year={2012}, month=oct, pages={1–33}, language={en} }","@misc{adult_2, author = {Becker, Barry and Kohavi, Ronny}, title = {{Adult}}, year = {1996}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5XW20}}","@misc{Larson_Angwin_Kirchner, title={How We Analyzed the COMPAS Recidivism Algorithm}, url={https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}, journal={ProPublica}, author={Larson, Jeff and Angwin, Julia and Kirchner, Lauren}, language={en} }","@misc{german_credit_data, author = {Hofmann, Hans}, title = {{Statlog (German Credit Data)}}, year = {1994}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5NC77}}"],Ni={filename:zi,metadata:Bi,description:Ei,pipeline:Gi,fairness:xi,implementation:Ki,use_cases:qi,bibliography:ji},Hi="FairlearnAdversarial",Ji={name:"Adversarial Mitigation",authors:"Fairlearn",version:"0.11.0",license:"MIT",source_paper:"\\textit{Mitigating Unwanted Biases with Adversarial Learning} \\cite{Zhang2018}"},Ui={method_type:["Adversarial Learning"],task:["Classification","Regression"],compatible_data:["Tabular Datasets"],method_description:"The predictor network is constructed to solve the underlying supervised learning task, without considering fairness, by minimizing the predictor loss. However, to improve fairness, we do not only minimize the predictor loss, but we also want to decrease the adversary’s ability to predict the sensitive features from the predictor’s predictions (when implementing demographic parity), or jointly from the predictor’s predictions and true labels (when implementing equalized odds)."},Wi={pipeline_location:"In-Processing",compatible_model:["Neural Networks"],model_description:"The neural network models cannot have an activation function or discrete prediction function on the final layer."},Xi={composition_features:["Categorical Attributes","Numerical Attribute"],fairness_guarantee:"Tunable Fairness Strength",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity","Equalized Odds"]}],fairness_description:"Fairness is achieved by minimizing the adversary's ability to predict the sensitive attribute. This is trade-off with the regular loss of the system, the strength of the fairness intervention can be tuned through a hyperparameter."},Yi={packages:[{programming_language:"Python",related_packages:["scikit-learn","PyTorch","Tensorflow Keras"]}],description:"The data needs to be provided as a 2D-arraylike of floats. The labels can be binary, categorical or float."},Vi={cases:["Adult \\cite{adult_2}"],description:""},Zi=[" @inproceedings{Zhang2018, author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret}, title = {Mitigating Unwanted Biases with Adversarial Learning}, year = {2018}, isbn = {9781450360128}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3278721.3278779}, doi = {10.1145/3278721.3278779}, booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society}, pages = {335–340}, numpages = {6}, keywords = {adversarial learning, debiasing, multi-task learning, unbiasing}, location = {New Orleans, LA, USA}, series = {AIES '18}}","@misc{adult_2, author = {Becker, Barry and Kohavi, Ronny}, title = {{Adult}}, year = {1996}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5XW20}}"],Qi={filename:Hi,metadata:Ji,description:Ui,pipeline:Wi,fairness:Xi,implementation:Yi,use_cases:Vi,bibliography:Zi},ea="FairlearnCorrelationRemover",ia={name:"Correlation Remover",authors:"Fairlearn",version:"0.11.0",license:"MIT",source_paper:""},aa={method_type:["Transformation"],task:["Task Independent"],compatible_data:["Tabular Datasets"],method_description:"Sensitive features can be correlated with non-sensitive features in the dataset. By applying the CorrelationRemover, these correlations are projected away while details from the original data are retained as much as possible (as measured by the least-squares error)."},ta={pipeline_location:"Pre-Processing",compatible_model:["Model Independent"],model_description:"We expect the CorrelationRemover to be most appropriate as a preprocessing step for (generalized) linear models."},na={composition_features:["Categorical Attributes"],fairness_guarantee:"No Fairness Guarantee",fairness_type_defs:[{fairness_type:"Dataset Unbiasing",fairness_definitions:["Removing Dataset Correlations"]}],fairness_description:"The degree to which the correlation is removed can be tuned with the $\\alpha$ parameter."},ra={packages:[{programming_language:"Python",related_packages:["scikit-learn","PyTorch","Tensorflow Keras"]}],description:""},sa={cases:["Diabetes 130-Hospitals \\cite{diabetes_130, Strack2014}"],description:""},oa=[" @article{Strack2014, title={Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records}, volume={2014}, rights={Copyright © 2014 Beata Strack et al.}, ISSN={2314-6141}, DOI={10.1155/2014/781670}, number={1}, journal={BioMed Research International}, author={Strack, Beata and DeShazo, Jonathan P. and Gennings, Chris and Olmo, Juan L. and Ventura, Sebastian and Cios, Krzysztof J. and Clore, John N.}, year={2014}, pages={781670}, language={en} }","@misc{diabetes_130, author = {Clore, John, Cios, Krzysztof, DeShazo, Jon, and Strack, Beata}, title = {{Diabetes 130-US Hospitals for Years 1999-2008}}, year = {2014}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5230J}}"],ca={filename:ea,metadata:ia,description:aa,pipeline:ta,fairness:na,implementation:ra,use_cases:sa,bibliography:oa},la="FairlearnReductions",da={name:"Reductions",authors:"Fairlearn",version:"0.11.0",license:"MIT",source_paper:"\\textit{A reductions approach to fair classification}. \\cite{agarwal18a} and \\\\ \\textit{Fair regression: quantitative definitions and reduction-based algorithms.} \\cite{agarwal19d}"},pa={method_type:["Constraint Optimization"],task:["Binary Classification","Regression"],compatible_data:["Tabular Datasets"],method_description:"A learning reduction takes as input complex examples, transforms them into simpler examples, invokes an appropriate learning algorithm on the simpler examples, then transforms predictions on these simpler examples to a prediction on the complex examples \\cite{Beygelzimer2016}.  A fairness constraint is used to transform a binary classification or regression problem to a cost-sensitive problem."},ha={pipeline_location:"In-Processing",compatible_model:["Probabilistic Classifier"],model_description:"Reductions is compatible with any underlying learner that can produce scores of predicted probabilities."},ma={composition_features:["Categorical Attributes"],fairness_guarantee:"Fairness Guaranteed",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity","Equalized Odds","Equal Opportunity","Predictive Equality","Overall Accuracy Equality","Bounded Group Loss"]}],fairness_description:"A difference bound is passed to the system which details the allowed fairness violation. The difference between two groups will be at most twice the value of this difference bound hyperparameter."},ua={packages:[{programming_language:"Python",related_packages:["scikit-learn","Tensorflow Keras"]}],description:"The reduction algorithms in Fairlearn only require a wrapper access to any 'base' learning algorithm, meaning that the 'base' algorithm only needs to implement fit and predict methods, as any standard scikit-learn estimator, but it does not need to have any knowledge of the desired fairness constraints or sensitive features."},ga={cases:["Credit Card Clients \\cite{default_of_credit_card_clients_350}"],description:""},fa=["@ARTICLE{Beygelzimer2016, author={Beygelzimer, Alina and Daumé, Hal and Langford, John and Mineiro, Paul}, journal={Proceedings of the IEEE}, title={Learning Reductions That Really Work}, year={2016},volume={104}, number={1}, pages={136-147}, keywords={Prediction algorithms;Big data;Classification algorithms;Error analysis;Computational modeling;Algorithm design and analysis;Learning systems;Learning systems;machine learning;prediction methods;Learning systems;machine learning;prediction methods}, doi={10.1109/JPROC.2015.2494118}}","@InProceedings{agarwal18a, title = {A Reductions Approach to Fair Classification}, author = {Agarwal, Alekh and Beygelzimer, Alina and Dudik, Miroslav and Langford, John and Wallach, Hanna}, booktitle = {Proceedings of the 35th International Conference on Machine Learning}, pages = {60--69}, year = {2018}, editor = {Dy, Jennifer and Krause, Andreas}, volume = {80}, series = {Proceedings of Machine Learning Research}, month = {10--15 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf}, url = {https://proceedings.mlr.press/v80/agarwal18a.html}}","@InProceedings{agarwal19d, title = {Fair Regression: Quantitative Definitions and Reduction-Based Algorithms}, author = {Agarwal, Alekh and Dudik, Miroslav and Wu, Zhiwei Steven}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {120--129}, year = {2019}, editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/agarwal19d/agarwal19d.pdf}, url = {https://proceedings.mlr.press/v97/agarwal19d.html}}","@misc{default_of_credit_card_clients_350, author = {Yeh, I-Cheng}, title = {{Default of Credit Card Clients}},year  = {2009}, howpublished = {UCI Machine Learning Repository}, note  = {{DOI}: https://doi.org/10.24432/C55S3H} }"],ya={filename:la,metadata:da,description:pa,pipeline:ha,fairness:ma,implementation:ua,use_cases:ga,bibliography:fa},_a="FairlearnThresholdOptimizer",ba={name:"Threshold Optimizer",authors:"Fairlearn",version:"0.11.0",license:"MIT",source_paper:"\\textit{Equality of Opportunity in Supervised Learning} \\cite{Hardt2016}"},Aa={method_type:["Thresholding"],task:["Binary Classification","Hard Labels"],compatible_data:["Dataset Independent"],method_description:"For each sensitive feature value, ThresholdOptimizer creates separate thresholds and applies them to the predictions of the user-provided estimator. To decide on the thresholds it generates all possible thresholds and selects the best combination in terms of the objective and the fairness constraints."},ka={pipeline_location:"Post-Processing",compatible_model:["Probabilistic Classifier"],model_description:"ThresholdOptimizer expects an estimator that provides it with scores. While the output of ThresholdOptimizer is binary, the input is not limited to scores derived from binary classifiers. In fact, real valued input, e.g. from a regressor, provides it with many more options to create thresholds."},wa={composition_features:["Categorical Attributes"],fairness_guarantee:"Fairness Guaranteed",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity","Equalized Odds","Equal Opportunity","Predictive Equality","Predictive Parity","False Omission Rate Parity"]}],fairness_description:"ThresholdOptimizer is built to satisfy the specified fairness criteria exactly and with no remaining disparity. In many cases this comes at the expense of performance, for example, with significantly lower accuracy. Regardless, it provides an interesting data point for comparison with other models."},va={packages:[{programming_language:"Python",related_packages:["scikit-learn"]}],description:"By default, ThresholdOptimizer trains the passed estimator using its fit() method. If prefit is set to True, ThresholdOptimizer does not call fit() on the estimator and assumes that it is already trained."},Ca={cases:["Diabetes 130-Hospitals \\cite{diabetes_130, Strack2014}"],description:""},Ia=[" @inproceedings{Hardt2016, title={Equality of Opportunity in Supervised Learning}, volume={29}, url={https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati}, year={2016}}"," @article{Strack2014, title={Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records}, volume={2014}, rights={Copyright © 2014 Beata Strack et al.}, ISSN={2314-6141}, DOI={10.1155/2014/781670}, number={1}, journal={BioMed Research International}, author={Strack, Beata and DeShazo, Jonathan P. and Gennings, Chris and Olmo, Juan L. and Ventura, Sebastian and Cios, Krzysztof J. and Clore, John N.}, year={2014}, pages={781670}, language={en} }","@misc{diabetes_130, author = {Clore, John, Cios, Krzysztof, DeShazo, Jon, and Strack, Beata}, title = {{Diabetes 130-US Hospitals for Years 1999-2008}}, year = {2014}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C5230J}}"],Pa={filename:_a,metadata:ba,description:Aa,pipeline:ka,fairness:wa,implementation:va,use_cases:Ca,bibliography:Ia},Fa="FairretProjection",Ra={name:"Fairret - Projection",authors:"Maarten Buyl, MaryBeth Defrance, and Tijl De Bie",version:"0.1.3",license:"MIT",source_paper:"\\textit{fairret: a Framework for Differentiable Fairness Regularization Terms} \\cite{buyl2024fairret}"},Da={method_type:["Regularization"],task:["Binary Classification","Soft Labels"],compatible_data:["Tabular Datasets","Image Datasets"],method_description:"A FAIRRET quantifies a model’s unfairness as a single value that is minimized like any other objective through automatic differentiation."},$a={pipeline_location:"In-Processing",compatible_model:["Neural Networks"],model_description:"The method is designed to be implemented in neural networks, however any method that uses gradients could be used."},Sa={composition_features:["Parallel Attributes","Numerical Attribute"],fairness_guarantee:"Tunable Fairness Strength",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity","Equal Opportunity","Predictive Equality","Predictive Parity","False Omission Rate Parity","Overall Accuracy Equality","Treatment Equality","F1-score Equality"]}],fairness_description:"The method calculates the loss term by determining the distance of the model to the c-fixed fair set of models. Instead of using the linear-fractional models directly, the measures are set to the average value of the measure in a batch, allowing to transform them to linear constraints. These linear constraints constitute the fair set. The method can account for numerical attributes, however linear behavior is expected."},Ta={packages:[{programming_language:"Python",related_packages:["PyTorch"]}],description:"The package is implemented to work with PyTorch. No experiments were conducted to confirm the performance on image datasets."},La={cases:["Bank \\cite{Moro_Cortez_Rita_2014}","CreditCard \\cite{Yeh_Lien_2009}","LawSchool (SEAPHE project)","ACS Datasets \\cite{ding2021retiring}"],description:""},Ma=["@inproceedings{buyl2024fairret, title={fairret: a Framework for Differentiable Fairness Regularization Terms}, author={Buyl, Maarten and Defrance, Marybeth and De Bie, Tijl}, booktitle={International Conference on Learning Representations}, year={2024}}","@article{Moro_Cortez_Rita_2014, title={A data-driven approach to predict the success of bank telemarketing}, volume={62}, ISSN={0167-9236}, DOI={10.1016/j.dss.2014.03.001},  journal={Decision Support Systems}, author={Moro, Sérgio and Cortez, Paulo and Rita, Paulo}, year={2014}, month=jun, pages={22–31} }","@article{Yeh_Lien_2009, title={The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients}, volume={36}, ISSN={0957-4174}, DOI={10.1016/j.eswa.2007.12.020}, number={2, Part 1}, journal={Expert Systems with Applications}, author={Yeh, I-Cheng and Lien, Che-hui}, year={2009}, month=mar, pages={2473–2480} }","@article{ding2021retiring, title={Retiring Adult: New Datasets for Fair Machine Learning}, author={Ding, Frances and Hardt, Moritz and Miller, John and Schmidt, Ludwig},journal={Advances in Neural Information Processing Systems},volume={34},year={2021}}"],Oa={filename:Fa,metadata:Ra,description:Da,pipeline:$a,fairness:Sa,implementation:Ta,use_cases:La,bibliography:Ma},za="FairretViolation",Ba={name:"Fairret - Violation",authors:"Maarten Buyl, MaryBeth Defrance, and Tijl De Bie",version:"0.1.3",license:"MIT",source_paper:"\\textit{fairret: a Framework for Differentiable Fairness Regularization Terms} \\cite{buyl2024fairret}"},Ea={method_type:["Regularization"],task:["Binary Classification","Soft Labels"],compatible_data:["Tabular Datasets","Image Datasets"],method_description:"A FAIRRET quantifies a model’s unfairness as a single value that is minimized like any other objective through automatic differentiation."},Ga={pipeline_location:"In-Processing",compatible_model:["Neural Networks"],model_description:"The method is designed to be implemented in neural networks, however any method that uses gradients could be used."},xa={composition_features:["Parallel Attributes","Numerical Attribute"],fairness_guarantee:"Tunable Fairness Strength",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity","Equal Opportunity","Predictive Equality","Predictive Parity","False Omission Rate Parity","Overall Accuracy Equality","Treatment Equality","F1-score Equality"]}],fairness_description:"A violation FAIRRET uses the proportion to which the fairness measure differs from the c-fixed measure as an added regularization term. This c-fixed measure is the average value for the measure within a batch, as this will be an integer it allows for the linear-fractional fairness measures to be transformed into linear equations. The method can account for numerical attributes, however linear behavior is expected."},Ka={packages:[{programming_language:"Python",related_packages:["PyTorch"]}],description:"The package is implemented to work with PyTorch. No experiments were conducted to confirm the performance on image datasets."},qa={cases:["Bank \\cite{Moro_Cortez_Rita_2014}","CreditCard \\cite{Yeh_Lien_2009}","LawSchool (SEAPHE project)","ACS Datasets \\cite{ding2021retiring}"],description:""},ja=["@inproceedings{buyl2024fairret, title={fairret: a Framework for Differentiable Fairness Regularization Terms}, author={Buyl, Maarten and Defrance, Marybeth and De Bie, Tijl}, booktitle={International Conference on Learning Representations}, year={2024}}","@article{Moro_Cortez_Rita_2014, title={A data-driven approach to predict the success of bank telemarketing}, volume={62}, ISSN={0167-9236}, DOI={10.1016/j.dss.2014.03.001},  journal={Decision Support Systems}, author={Moro, Sérgio and Cortez, Paulo and Rita, Paulo}, year={2014}, month=jun, pages={22–31} }","@article{Yeh_Lien_2009, title={The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients}, volume={36}, ISSN={0957-4174}, DOI={10.1016/j.eswa.2007.12.020}, number={2, Part 1}, journal={Expert Systems with Applications}, author={Yeh, I-Cheng and Lien, Che-hui}, year={2009}, month=mar, pages={2473–2480} }","@article{ding2021retiring, title={Retiring Adult: New Datasets for Fair Machine Learning}, author={Ding, Frances and Hardt, Moritz and Miller, John and Schmidt, Ludwig},journal={Advances in Neural Information Processing Systems},volume={34},year={2021}}"],Na={filename:za,metadata:Ba,description:Ea,pipeline:Ga,fairness:xa,implementation:Ka,use_cases:qa,bibliography:ja},Ha="OxonFair",Ja={name:"OxonFair",authors:"E. Delaney, Z. Fu, S. Wachter, B. Mittelstadt, and C. Russell",version:"NA",license:"Apache-2.0 license",source_paper:"\\textit{OxonFair: A Flexible Toolkit for Algorithmic Fairness} \\cite{Delaney2024}"},Ua={method_type:["Thresholding"],task:["Classification","Hard Labels","Inferring Sensitive Attributes"],compatible_data:["Dataset Independent"],method_description:"OxonFair is a postprocessing approach for enforcing fairness, with support for a wide range of performance metrics and fairness criteria, and support for inferred attributes, i.e., it does not require access to protected attributes at test time. Under the hood, FairPredictor works by adjusting the decision boundary for each group individually. Where groups are not available, it makes use of inferred group membership to adjust decision boundaries."},Wa={pipeline_location:"Post-Processing",compatible_model:["Probabilistic Classifier"],model_description:"Thresholding can be applied to most pretrained ML algorithms, and optimal thresholds can be selected using held-out validation data unused in training. "},Xa={composition_features:["Categorical Attributes","No Attributes at Inference"],fairness_guarantee:"Fairness Guaranteed",fairness_type_defs:[{fairness_type:"Group Fairness",fairness_definitions:["Demographic Parity","Equal Opportunity","Predictive Equality","Predictive Parity","False Omission Rate Parity","Overall Accuracy Equality","F1-score Equality","Min-Max"]}],fairness_description:"OxonFair supports any fairness measure (including conditional fairness measures) that can be expressed per group as a weighted sum of True Positives, False Positives, True Negatives and False Negatives. OxenFair selects the solution that best optimizes the objective (performance) while satisfying the constraint (fairness bound)."},Ya={packages:[{programming_language:"Python",related_packages:["scikit-learn","AutoGluon \\cite{agtabular}","PyTorch","XGBoost \\cite{Chen_Guestrin_2016}"]}],description:"Oxonfair has its own dataloader object. A notebook is available detailing how to add a dataset not yet in OxonFair. "},Va={cases:["COMPAS \\cite{Larson_Angwin_Kirchner}","CelebA \\cite{Liu_Luo_Wang_Tang_2015}","Multilingual Twitter corpus \\cite{Huang_Xing_Dernoncourt_Paul_2020}","Jigsaw \\cite{Jigsaw}","Myocardial Infarction \\cite{myocardial_infarction_complications_579}","XGBoost \\cite{Chen_Guestrin_2016}"],description:"Oxonfair has shown to work with a Resnet-50 backbone \\cite{He_Zhang_Ren_Sun_2016} trained on ImageNet \\cite{Deng_Dong_Socher_Li_Li_Fei2009}, decision trees, random forests, and XGBoost \\cite{Chen_Guestrin_2016}. "},Za=["@article{Delaney2024, title={OxonFair: A Flexible Toolkit for Algorithmic Fairness}, url={http://arxiv.org/abs/2407.13710}, DOI={10.48550/arXiv.2407.13710}, note={arXiv:2407.13710 [cs]}, number={arXiv:2407.13710}, publisher={arXiv}, author={Delaney, Eoin and Fu, Zihao and Wachter, Sandra and Mittelstadt, Brent and Russell, Chris}, year={2024}, month=nov }","@article{agtabular, title={AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data}, author={Erickson, Nick and Mueller, Jonas and Shirkov, Alexander and Zhang, Hang and Larroy, Pedro and Li, Mu and Smola, Alexander}, journal={arXiv preprint arXiv:2003.06505}, year={2020}}","@inproceedings{Liu_Luo_Wang_Tang_2015, title={Deep Learning Face Attributes in the Wild}, ISSN={2380-7504}, url={https://ieeexplore.ieee.org/document/7410782}, DOI={10.1109/ICCV.2015.425}, booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, author={Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou}, year={2015}, month=dec, pages={3730–3738} }","@inproceedings{Huang_Xing_Dernoncourt_Paul_2020, address={Marseille, France}, title={Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition}, ISBN={979-10-95546-34-4}, url={https://aclanthology.org/2020.lrec-1.180}, booktitle={Proceedings of the Twelfth Language Resources and Evaluation Conference}, publisher={European Language Resources Association}, author={Huang, Xiaolei and Xing, Linzi and Dernoncourt, Franck and Paul, Michael J.}, editor={Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios}, year={2020}, month=may, pages={1440–1448}, language={English} }","@misc{Jigsaw, title=Jigsaw Unintended Bias in Toxicity Classification, year=2019, url={https://kaggle.com/jigsaw-unintended-bias-in-toxicity-classification}, abstractNote={Detect toxicity across a diverse range of conversations}, language={en} }","@misc{myocardial_infarction_complications_579, author = {Golovenkin, S.E., Shulman, V.A., Rossiev, D.A., Shesternya, P.A., Nikulina, S.Yu., Orlova, Yu.V., and Voino-Yasenetsky, V.F.}, title = {{Myocardial infarction complications}}, year  = {2020}, howpublished = {UCI Machine Learning Repository}, note = {{DOI}: https://doi.org/10.24432/C53P5M}}","@inproceedings{Chen_Guestrin_2016, address={New York, NY, USA}, series={KDD ’16}, title={XGBoost: A Scalable Tree Boosting System}, ISBN={978-1-4503-4232-2}, url={https://dl.acm.org/doi/10.1145/2939672.2939785}, DOI={10.1145/2939672.2939785}, booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, publisher={Association for Computing Machinery}, author={Chen, Tianqi and Guestrin, Carlos}, year={2016}, month=aug, pages={785–794}, collection={KDD ’16} }","@inproceedings{He_Zhang_Ren_Sun_2016, title={Deep Residual Learning for Image Recognition}, ISSN={1063-6919}, url={https://ieeexplore.ieee.org/document/7780459}, DOI={10.1109/CVPR.2016.90}, booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}, year={2016}, month=jun, pages={770–778} }","@inproceedings{Deng_Dong_Socher_Li_Li_Fei2009, title={ImageNet: A large-scale hierarchical image database}, ISSN={1063-6919}, url={https://ieeexplore.ieee.org/document/5206848}, DOI={10.1109/CVPR.2009.5206848}, booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li}, year={2009}, month=jun, pages={248–255} }","@misc{Larson_Angwin_Kirchner, title={How We Analyzed the COMPAS Recidivism Algorithm}, url={https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}, journal={ProPublica}, author={Larson, Jeff and Angwin, Julia and Kirchner, Lauren}, language={en} }"],Qa={filename:Ha,metadata:Ja,description:Ua,pipeline:Wa,fairness:Xa,implementation:Ya,use_cases:Va,bibliography:Za},et=[y,P,O,N,Q,ce,ye,Pe,Oe,Ne,Qe,ci,yi,Pi,Oi,Ni,c,Qi,ca,ya,Pa,Oa,Na,Qa];export{et as a};
